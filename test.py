# FastAPI Fashion Recommender Application
# This API implements the "Filter then Rank" system and LLM-based tagging.

import os
import numpy as np
import tensorflow as tf
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from PIL import Image, UnidentifiedImageError
from functools import lru_cache
import base64
import io
import requests
import json

# --- 1. Application Setup ---

app = FastAPI(
    title="AI Fashion Advisor API",
    description="Provides fashion recommendations using a hybrid Filter-then-Rank system.",
)

# --- CORS Middleware ---
# Allows your React frontend to communicate with this API
origins = [
    "http://localhost:3000",  # Allow local React dev server
    # Add your deployed Vercel/frontend URL here later
    # "https://your-frontend-app.vercel.app",
]
app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- 2. Model and Data Loading ---

# --- Siamese Model Loading ---
try:
    ENCODER_MODEL = tf.keras.models.load_model('fashion_compatibility_encoder.h5', compile=False)
    print("✅ Siamese encoder model loaded successfully.")
except IOError:
    print("❌ CRITICAL ERROR: 'fashion_compatibility_encoder.h5' not found.")
    ENCODER_MODEL = None

# --- Gemini LLM Configuration ---
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY", "YOUR_API_KEY_HERE")
if GEMINI_API_KEY == "YOUR_API_KEY_HERE":
    print("⚠️ WARNING: Using a placeholder Gemini API key. Please set your environment variable.")
GEMINI_API_URL = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={GEMINI_API_KEY}"

# --- Mock Database ---
# In a real app, this would be Firestore, MongoDB, etc.
# This data should be generated by your one-time "bootstrapping" script.
WARDROBE_DB = [
    {'id': 'top1', 'image_path': 'wardrobe/top1.jpg', 'category': 'top', 'tags': {'season': ['summer'], 'occasion': ['casual']}},
    {'id': 'top2', 'image_path': 'wardrobe/top2.jpg', 'category': 'top', 'tags': {'season': ['all-season'], 'occasion': ['work', 'formal']}},
    {'id': 'top3', 'image_path': 'wardrobe/top3.jpg', 'category': 'top', 'tags': {'season': ['winter'], 'occasion': ['party', 'casual']}},
    {'id': 'bottom1', 'image_path': 'wardrobe/bottom1.jpg', 'category': 'bottom', 'tags': {'season': ['summer', 'spring'], 'occasion': ['casual']}},
    {'id': 'bottom2', 'image_path': 'wardrobe/bottom2.jpg', 'category': 'bottom', 'tags': {'season': ['all-season'], 'occasion': ['work', 'formal']}},
    {'id': 'shoes1', 'image_path': 'wardrobe/shoes1.jpg', 'category': 'shoes', 'tags': {'season': ['summer'], 'occasion': ['casual']}},
]

# --- 3. Helper Functions ---

IMG_SHAPE = (128, 128, 3)

def preprocess_image(image_bytes):
    """Loads and preprocesses an image from bytes."""
    try:
        img = Image.open(io.BytesIO(image_bytes)).convert('RGB')
        img = img.resize((IMG_SHAPE[0], IMG_SHAPE[1]))
        img = np.array(img) / 255.0
        return np.expand_dims(img, axis=0)
    except (IOError, UnidentifiedImageError):
        return None

@lru_cache(maxsize=None)
def get_embedding(item_id):
    """Computes and caches an embedding for a given item ID from the DB."""
    if not ENCODER_MODEL: return None
    item = next((item for item in WARDROBE_DB if item['id'] == item_id), None)
    if not item: return None
    
    try:
        with open(item['image_path'], "rb") as f:
            image_bytes = f.read()
        processed_img = preprocess_image(image_bytes)
        if processed_img is None: return None
        embedding = ENCODER_MODEL.predict(processed_img, verbose=0)
        return embedding
    except FileNotFoundError:
        return None

def get_compatibility_score(embedding1, embedding2):
    """Calculates the euclidean distance between two embeddings."""
    return np.linalg.norm(embedding1 - embedding2)

# Pre-compute all embeddings on startup for faster ranking
ITEM_EMBEDDINGS = {}
def compute_all_embeddings():
    print("Pre-computing all item embeddings...")
    for item in WARDROBE_DB:
        ITEM_EMBEDDINGS[item['id']] = get_embedding(item['id'])
    print("✅ All embeddings computed and cached.")

# --- 4. API Endpoints ---

@app.on_event("startup")
def startup_event():
    # Create dummy images if they don't exist for testing
    if not os.path.exists('wardrobe'):
        os.makedirs('wardrobe')
    for item in WARDROBE_DB:
        if not os.path.exists(item['image_path']):
            Image.new('RGB', (100, 100), color='red').save(item['image_path'])
    compute_all_embeddings()

@app.get("/")
def read_root():
    return {"message": "Welcome to the AI Fashion Advisor API"}

# --- LLM Tagging Endpoint ---
@app.post("/tag-item")
async def tag_item_with_llm(file: UploadFile = File(...)):
    """
    Accepts an image upload, sends it to the Gemini LLM, and returns
    predicted structured tags for the clothing item.
    """
    if GEMINI_API_KEY == "YOUR_API_KEY_HERE":
        raise HTTPException(status_code=500, detail="Gemini API key not configured on the server.")

    image_bytes = await file.read()
    base64_image = base64.b64encode(image_bytes).decode('utf-8')

    prompt_parts = [
        {
            "text": """You are a fashion expert metadata tagger. Analyze the clothing item in the image and return a single, valid JSON object. The object must have the following keys:
- "category": A string, one of ["top", "bottom", "shoes", "outerwear", "accessory"].
- "type": A specific string describing the item (e.g., "t-shirt", "blouse", "jeans", "sneakers").
- "season": An array of strings, choose from ["spring", "summer", "autumn", "winter"].
- "occasion": An array of strings, choose from ["casual", "work", "formal", "party", "sporty"].
Do not include any text or markdown formatting before or after the JSON object."""
        },
        {
            "inline_data": {
                "mime_type": "image/jpeg",
                "data": base64_image
            }
        }
    ]
    
    payload = {"contents": [{"parts": prompt_parts}], "generationConfig": {"response_mime_type": "application/json"}}
    headers = {'Content-Type': 'application/json'}

    try:
        response = requests.post(GEMINI_API_URL, json=payload, headers=headers)
        response.raise_for_status()
        result_json = response.json()['candidates'][0]['content']['parts'][0]['text']
        return json.loads(result_json)
    except requests.exceptions.RequestException as e:
        raise HTTPException(status_code=500, detail=f"Failed to call Gemini API: {e}")
    except (KeyError, json.JSONDecodeError) as e:
        raise HTTPException(status_code=500, detail=f"Could not parse LLM response: {e}")


# --- Filter-then-Rank Recommendation Endpoint ---
class RecommendationRequest(BaseModel):
    input_item_id: str
    filters: dict # e.g., {"season": "summer", "occasion": "casual"}

@app.post("/recommendations/filtered")
def get_filtered_recommendations(request: RecommendationRequest):
    """
    Implements the Filter-then-Rank logic.
    1. Filters the wardrobe based on tags.
    2. Ranks the filtered results using the Siamese model's compatibility score.
    """
    if not ENCODER_MODEL:
        raise HTTPException(status_code=500, detail="Siamese model is not available.")

    # --- Stage 1: Filtering ---
    filtered_candidates = WARDROBE_DB
    for key, value in request.filters.items():
        # This logic handles filtering for tags within a nested dictionary
        filtered_candidates = [
            item for item in filtered_candidates
            if value in item.get('tags', {}).get(key, [])
        ]
    
    # Exclude the input item itself
    filtered_candidates = [item for item in filtered_candidates if item['id'] != request.input_item_id]

    if not filtered_candidates:
        return {"message": "No items found matching your filters.", "recommendations": []}

    # --- Stage 2: Ranking ---
    input_embedding = ITEM_EMBEDDINGS.get(request.input_item_id)
    if input_embedding is None:
        raise HTTPException(status_code=404, detail="Input item not found or could not be processed.")

    recommendations = []
    for candidate in filtered_candidates:
        candidate_embedding = ITEM_EMBEDDINGS.get(candidate['id'])
        if candidate_embedding is not None:
            score = get_compatibility_score(input_embedding, candidate_embedding)
            recommendations.append({"item": candidate, "compatibility_score": float(score)})

    # Sort by score (lower is better)
    recommendations.sort(key=lambda x: x['compatibility_score'])
    
    return {"recommendations": recommendations[:10]} # Return top 10
